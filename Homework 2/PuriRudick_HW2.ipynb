{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "*Author: Puri Rudick*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in three books (same as HW1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the txt files\n",
    "books = {\n",
    "    '''The Child's World: Third Reader by Browne, Tate, and Withers''' : 'https://www.gutenberg.org/cache/epub/15170/pg15170.txt',\n",
    "    'Fourth Reader: The Alexandra Readers by Dearness, McIntyre, and Saul' : 'https://www.gutenberg.org/files/51975/51975-0.txt',\n",
    "    'School Reading By Grades: Fifth Year by James Baldwin' : 'https://www.gutenberg.org/files/51000/51000-0.txt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_book(url):\n",
    "    response = request.urlopen(url)\n",
    "    text = response.read().decode('utf8')\n",
    "    \n",
    "    # Find the beginning and the end of the book to remove Meta\n",
    "    start2 = text.find('Produced by')\n",
    "    end = text.find('END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "    text = text[start2-11:end]\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = read_in_book(list(books.values())[0])\n",
    "b2 = read_in_book(list(books.values())[1])\n",
    "b3 = read_in_book(list(books.values())[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vocabulary Size: Score and Normalize Score\n",
    "\n",
    "In Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. \n",
    "Some relevant resources that you can leverage:\n",
    "- https://docs.tibco.com/pub/spotfire/6.5.0/doc/html/norm/norm_scale_between_0_and_1.htm\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "\n",
    "Normaliztion can be done using the simple normalization formula. The formula is dividing the difference of each text's vocabulary size to the min vocabular size on the list by the difference between the max and the min vocabulary size. Texts from nltk package were used. It can also be done using Min Max Scaler from the sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Method to get the Vocab Size and normalize the score\n",
    "\n",
    "def n_vocab_size(*arg):\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        vocab_size = np.append(vocab_size,len(set(text)))\n",
    "    \n",
    "    #### Normalizing using the formula \n",
    "    for vsize in vocab_size:\n",
    "        vocab_size_norm = np.append(vocab_size_norm,(vsize - vocab_size.min()) / (vocab_size.max() - vocab_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    vocab_size_norm_sklearn = minmax_scale(vocab_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size,vocab_size_norm,vocab_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:\n",
      "\t- 4625.0\n",
      "\t- 11294.0\n",
      "\t- 8372.0\n",
      "Normalization using the simple formula:\n",
      "\t- 0.0\n",
      "\t- 1.0\n",
      "\t- 0.5618533513270355\n",
      "Normalization using the sklearn module:\n",
      "\t- 0.0\n",
      "\t- 0.9999999999999999\n",
      "\t- 0.5618533513270355\n"
     ]
    }
   ],
   "source": [
    "vocab_size = n_vocab_size(b1,b2,b3)\n",
    "\n",
    "print(\"Vocabulary Size:\", *vocab_size[0],sep='\\n\\t- ')\n",
    "print(\"Normalization using the simple formula:\", *vocab_size[1],sep='\\n\\t- ')\n",
    "print(\"Normalization using the sklearn module:\", *vocab_size[2],sep='\\n\\t- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Long-word Vocabulary: Score and Normalize Score\n",
    "After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n",
    "\n",
    "Below we used the same normalization methods as we used with the overall vocabulary size. Using the texts from nltk package we computed the long word vocabulary size. The function written below computes the values and normalization values of the long word vocabulary sizes of multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_long_vocab_size(*arg, longWordLength = 10):\n",
    "    v_size = np.array([])\n",
    "    v_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        V = set(text)\n",
    "        long_words = [w for w in V if len(w) > longWordLength]\n",
    "        v_size = np.append(v_size,len(set(long_words)))\n",
    "    \n",
    "    #### Normalizing using formula \n",
    "    for vsize in v_size:\n",
    "        v_size_norm = np.append(v_size_norm,(vsize - v_size.min()) /\n",
    "                                                    (v_size.max() - v_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn module\n",
    "    v_size_norm_sklearn = minmax_scale(v_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(v_size,v_size_norm,v_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Vocabulary Size:\n",
      "\t- 106.0\n",
      "\t- 633.0\n",
      "\t- 399.0\n",
      "Normalization using the simple formula\n",
      "\t- 0.0\n",
      "\t- 1.0\n",
      "\t- 0.5559772296015181\n",
      "Normalization using the sklearn module\n",
      "\t- 0.0\n",
      "\t- 1.0\n",
      "\t- 0.555977229601518\n"
     ]
    }
   ],
   "source": [
    "long_vocab_size = n_long_vocab_size(b1,b2,b3)\n",
    "\n",
    "print(\"Long Vocabulary Size:\", *long_vocab_size[0],sep='\\n\\t- ')\n",
    "print(\"Normalization using the simple formula\", *long_vocab_size[1],sep='\\n\\t- ')\n",
    "print(\"Normalization using the sklearn module\", *long_vocab_size[2],sep='\\n\\t- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Difficulty Score\n",
    "\n",
    "Now create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical_diversity function from nltk book\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def txt_difficulty_score(*arg, lex_div_weight=1, long_vocab_size_weight=1, vocab_size_weight=1):\n",
    "\n",
    "    lex_div_score = np.array([])\n",
    "\n",
    "    #### Getting the lexical diversity\n",
    "    for text in arg:\n",
    "        lex_div_score = np.append(lex_div_score,lexical_diversity(text))\n",
    "    long_vocab_size = n_long_vocab_size(*arg)\n",
    "    vocab_size = n_vocab_size(*arg)\n",
    "\n",
    "    lex_div_score = lex_div_score * lex_div_weight\n",
    "    # print(lex_div_score)\n",
    "    vocab_size = vocab_size[1] * vocab_size_weight\n",
    "    # print(vocab_size)\n",
    "    long_vocab_size = long_vocab_size[1] * long_vocab_size_weight\n",
    "    # print(long_vocab_size)\n",
    "    \n",
    "    txt_diff_score = (lex_div_score + long_vocab_size + vocab_size) / 3\n",
    "    \n",
    "    return(txt_diff_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Difficulty Score for The Child's World: Third Reader by Browne, Tate, and Withers is  0.035379613692866706\n",
      "Text Difficulty Score for Fourth Reader: The Alexandra Readers by Dearness, McIntyre, and Saul is  0.702631612467678\n",
      "Text Difficulty Score for School Reading By Grades: Fifth Year by James Baldwin is  0.42643003987186195\n"
     ]
    }
   ],
   "source": [
    "txt_difficulty = txt_difficulty_score(b1,b2,b3)\n",
    "\n",
    "for i,j in zip(txt_difficulty,books):\n",
    "    print(\"Text Difficulty Score for\", j, \"is \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The 3 books I used in this experiment are for 3rd, 4th, and 5th grade readers.  They are from 3 different authors, also different genes. From the result above I found that normalization can be done using the simple normalization formula or Min Max Scaler from the sklearn module.  Both methods give the same results.\n",
    "\n",
    "For vocabulary and long vocabulary size, 3rd grade book has the lowest, follows by 5th grade book, and 4th grade book has the highest. The text difficulty score also shows the same trend. The 3rd grade book has slowest score, which makes sense becuase the book for the youngest childrend (among these 3 books) should be the easiest. However, the 4th grade book has higher text difficulty than 5th grade book. These results coresponse with the trend that I've seen from the Homework 1.\n",
    "\n",
    "The reason behind this maybe becuase I choose 3 different authors and genes, each authors might have different views of how difficult of books for each reader grade should be.  If these books come from the same author, I think it text difficulty should be higher with the higher age of readers."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48e120922dd8eb9d261b39c8c0044617a1d8fb07c931ac347bc85fe451298055"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
