{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "*Author: Puri Rudick*\n",
    "\n",
    "1.\tCompare your given name with your nickname (if you don’t have a nickname, invent one for this assignment) by answering the following questions:\n",
    "- a.\tWhat is the edit distance between your nickname and your given name?\n",
    "- b.\tWhat is the percentage string match between your nickname and your given name?\n",
    "\n",
    "Show your work for both calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance - NLTK function:  8\n",
      "Percentage String Match: 0.17%\n"
     ]
    }
   ],
   "source": [
    "given_name = 'Wiphawan'\n",
    "nickname = 'Puri'\n",
    "\n",
    "def match(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "print('Edit Distance - NLTK function: ', nltk.edit_distance(given_name, nickname))\n",
    "print('Percentage String Match: {:.2f}%'.format(SequenceMatcher(None, given_name, nickname).ratio()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit Distance: As expected since my given name (8 letters) and nickname (4 letter) share 2 letters => (8-2)+(4-2)**\n",
    "\n",
    "**Percentage String Match: 2 Lettes match from total of 8+2 = 12 => 2/12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tFind a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think you friend either was or was not able to guess the book from hearing the list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ความมหัศจรรย์ของหนังสือเล่มนี้คือ', 'บางบทเราจะคิดถึงคนอื่น']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myThaiBook = 'ความมหัศจรรย์ของหนังสือเล่มนี้คือ บางบทเราจะคิดถึงคนอื่น'\n",
    "myThaiBook = word_tokenize(myThaiBook)\n",
    "myThaiBook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I first tried a Thai book. These 2 sentences above have 6 and 7 words.  Obviously, nltk is not made for Thai :')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentences come from Educated - Tara Westover\n",
    "educated = 'Charles was my first friend from that other world, the one my father had tried to protect me from. He was conventional in all the ways and for all the reasons my father despised conventionally.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences:\n",
      " Charles was my first friend from that other world, the one my father had tried to protect me from. He was conventional in all the ways and for all the reasons my father despised conventionally.\n",
      "\n",
      "Sentences after removing stop words:\n",
      " ['Charles', 'first', 'friend', 'world', ',', 'one', 'father', 'tried', 'protect', '.', 'He', 'conventional', 'ways', 'reasons', 'father', 'despised', 'conventionally', '.']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(txt):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(txt) \n",
    "    wo_stopwords_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    wo_stopwords_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            wo_stopwords_sentence.append(w) \n",
    "    return wo_stopwords_sentence\n",
    "\n",
    "educated_wo_stopwords = remove_stopwords(educated)\n",
    "print('Original sentences:\\n', educated)\n",
    "print('\\nSentences after removing stop words:\\n', educated_wo_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Educated was on of the books that both my husband and I read. He was able to guess that the sentences are from the book. I believe this is because removing stop words still preserve most of the words in the sentence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tRun one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences:\n",
      " ['charl', 'wa', 'my', 'first', 'friend', 'from', 'that', 'other', 'world', ',', 'the', 'one', 'my', 'father', 'had', 'tri', 'to', 'protect', 'me', 'from', '.', 'he', 'wa', 'convent', 'in', 'all', 'the', 'way', 'and', 'for', 'all', 'the', 'reason', 'my', 'father', 'despis', 'convent', '.']\n",
      "\n",
      "Sentences after removing stop words:\n",
      " ['charl', 'first', 'friend', 'world', ',', 'one', 'father', 'tri', 'protect', '.', 'he', 'convent', 'way', 'reason', 'father', 'despis', 'convent', '.']\n"
     ]
    }
   ],
   "source": [
    "def StemViaPorter(txt):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(txt)\n",
    "\n",
    "    stem_words = []\n",
    "    for w in words:\n",
    "        stem_words.append(ps.stem(w))\n",
    "    return(stem_words)\n",
    "\n",
    "def StemViaPorterwoStopWords(text):\n",
    "    ps = PorterStemmer()\n",
    "    stem_words = []\n",
    "    for w in text:\n",
    "        stem_words.append(ps.stem(w))\n",
    "    return(stem_words)\n",
    "\n",
    "print('Original sentences:\\n', StemViaPorter(educated))\n",
    "print('\\nSentences after removing stop words:\\n', StemViaPorterwoStopWords(educated_wo_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences have 80.83% of stem words as real word.\n",
      "Sentences after removing stop words have 72.22% of stem words as real word.\n"
     ]
    }
   ],
   "source": [
    "all_words = set(words.words())\n",
    "\n",
    "Snowball = SnowballStemmer(\"english\")\n",
    "original_stems = [Snowball.stem(i) for i in educated]\n",
    "wo_stopwords_stems = [Snowball.stem(i) for i in educated_wo_stopwords]\n",
    "\n",
    "valid_counts = 0\n",
    "\n",
    "def words_count(stem):\n",
    "    valid_counts = 0\n",
    "    for i in stem:\n",
    "        if i in all_words:\n",
    "            valid_counts += 1\n",
    "    return valid_counts\n",
    "    \n",
    "frmt = \"{:.2f}% of words stemmed from the first two sentences of the Book of Revelation using a Snowball Stemmer are valid roots\"\n",
    "original_stem_words_count = words_count(original_stems)\n",
    "wo_stopwords_stem_words_count = words_count(wo_stopwords_stems)\n",
    "\n",
    "print('Original sentences have {:.2f}% of stem words as real word.'.format(original_stem_words_count / len(original_stems)*100))\n",
    "print('Sentences after removing stop words have {:.2f}% of stem words as real word.'.format(wo_stopwords_stem_words_count / len(wo_stopwords_stems)*100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48e120922dd8eb9d261b39c8c0044617a1d8fb07c931ac347bc85fe451298055"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
